{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c8d6158-debf-425d-b7e7-2ebdfce2e406",
    "_uuid": "5d6ba4cbea1a979cdfda17b17e4abf216ba9f435"
   },
   "source": [
    "**Predict Diabetes From Medical Records**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90395f0",
   "metadata": {},
   "source": [
    "# Predict Diabetes From Medical Records\n",
    "\n",
    "Diabetes mellitus (DM), commonly referred to as diabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Type 1 diabetes results from the pancreas' failure to produce enough insulin. Type 2 diabetes begins with insulin resistance, a condition in which cells fail to respond to insulin properly. As of 2015, an estimated 415 million people had diabetes worldwide, with type 2 diabetes making up about 90% of the cases. This represents 8.3% of the adult population. \n",
    "\n",
    "The [Pima Indians Diabetes Database](https://www.kaggle.com/uciml/pima-indians-diabetes-database) can be used to train machine learning models to predict if a given patient has diabetes. This dataset contains measurements relating to Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, and Age.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "Diabetes mellitus (DM), commonly referred to as diabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period.  Type 1 diabetes results from the pancreas's failure to produce enough insulin.  Type 2 diabetes begins with insulin resistance, a condition in which cells fail to respond to insulin properly.  As of 2015, an estimated 415 million people had diabetes worldwide, with type 2 diabetes making up about 90% of the cases. This represents 8.3% of the adult population.    Source: https://en.wikipedia.org/wiki/Diabetes_mellitus\n",
    "\n",
    "The [Pima Indians Diabetes Database](https://www.kaggle.com/uciml/pima-indians-diabetes-database) can be used to train machine learning models to predict if a given patient has diabetes.  This dataset contains measurements relating to Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, and Age.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d146b3",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "In this section, we will load the dataset and perform some initial exploratory data analysis to understand the structure and characteristics of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6950f5fb-7d29-404a-9de1-cfe88e801008",
    "_uuid": "46aaadc5a4b6db28876d62cc5131e11ac8022b80"
   },
   "source": [
    "*Step 1: Import Python Packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "37867649-a1fc-44f7-acd7-9794152a9c04",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8a89dacf039a9f136a8d06799ffe5068d1a60e39",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import graphviz \n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, learning_curve, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score \n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21fb54",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We will handle missing values, scale the features, and split the dataset into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f6fb117-9aa2-4182-b1c8-529c3bfdf3c9",
    "_uuid": "2eea7459357647b00d7c58930db769ec3c843873"
   },
   "source": [
    "*Step 2: Define Helper Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "f42887f6-2a04-4eb4-a912-764a05776d64",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "89fb0cd2287578fec1c291e1d610d0a9116663b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def compareABunchOfDifferentModelsAccuracy(a, b, c, d):\n",
    "    \"\"\"\n",
    "    compare performance of classifiers on X_train, X_test, Y_train, Y_test\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "    http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n",
    "    \"\"\"    \n",
    "    print('\\nCompare Multiple Classifiers: \\n')\n",
    "    print('K-Fold Cross-Validation Accuracy: \\n')\n",
    "    names = []\n",
    "    models = []\n",
    "    resultsAccuracy = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('SVM', SVC()))\n",
    "    models.append(('LSVM', LinearSVC()))\n",
    "    models.append(('GNB', GaussianNB()))\n",
    "    models.append(('DTC', DecisionTreeClassifier()))\n",
    "    models.append(('GBC', GradientBoostingClassifier()))\n",
    "    for name, model in models:\n",
    "        model.fit(a, b)\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "        accuracy_results = model_selection.cross_val_score(model, a,b, cv=kfold, scoring='accuracy')\n",
    "        resultsAccuracy.append(accuracy_results)\n",
    "        names.append(name)\n",
    "        accuracyMessage = \"%s: %f (%f)\" % (name, accuracy_results.mean(), accuracy_results.std())\n",
    "        print(accuracyMessage) \n",
    "    # Boxplot\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison: Accuracy')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(resultsAccuracy)\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_ylabel('Cross-Validation: Accuracy Score')\n",
    "    plt.show()    \n",
    "      \n",
    "def defineModels():\n",
    "    print('\\nLR = LogisticRegression')\n",
    "    print('RF = RandomForestClassifier')\n",
    "    print('KNN = KNeighborsClassifier')\n",
    "    print('SVM = Support Vector Machine SVC')\n",
    "    print('LSVM = LinearSVC')\n",
    "    print('GNB = GaussianNB')\n",
    "    print('DTC = DecisionTreeClassifier')\n",
    "    print('GBC = GradientBoostingClassifier \\n\\n')\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"MLPClassifier\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]    \n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(kernel=\"linear\"),\n",
    "    SVC(kernel=\"rbf\"),\n",
    "    GaussianProcessClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "\n",
    "dict_characters = {0: 'Healthy', 1: 'Diabetes'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5d9edef2-3ae8-4894-b3c5-027f7b4a653b",
    "_uuid": "81f983b2cd7c874506495c507542a155f331fac2"
   },
   "source": [
    "*Step 3: Inspect and Clean Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fc7be",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "We will train multiple classification models and evaluate their performance using various metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1764e55-aae5-4959-9c67-5f3ea5203188",
    "_uuid": "8c19e0bf65102ac0c578cdac61380897ec2bb92e"
   },
   "source": [
    "When starting a new data analysis project it is important to inspect, understand, and clean the data.  When inspecting the first ten rows of data one thing that jumps out right away is that there are measurements of zero for both insulin levels and skin thickness.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "65475636-e376-491c-9ff5-f558598afd37",
    "_uuid": "924edc193c853395713bcfdade0a26f36633bf03"
   },
   "outputs": [],
   "source": [
    "dataset = read_csv('../input/diabetes.csv')\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67fd6314-6176-4449-9839-b209ab0bd373",
    "_uuid": "3c1e827efa192861b80ea25355106d6600ef9273"
   },
   "source": [
    "It would be a serious medical problem if a patient had an insulin level and skin thickness measurement of zero.  As such, we can conclude that this dataset uses the number zero to represent missing or null data.  Here we can see that as many as half of the rows contain columns with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "033d06fa-6a63-4c88-8745-6ab0eb9dc80a",
    "_uuid": "d87a65f1087b89f570fc14ffc96115577e6824bf"
   },
   "outputs": [],
   "source": [
    "def plotHistogram(values,label,feature,title):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plotOne = sns.FacetGrid(values, hue=label,aspect=2)\n",
    "    plotOne.map(sns.distplot,feature,kde=False)\n",
    "    plotOne.set(xlim=(0, values[feature].max()))\n",
    "    plotOne.add_legend()\n",
    "    plotOne.set_axis_labels(feature, 'Proportion')\n",
    "    plotOne.fig.suptitle(title)\n",
    "    plt.show()\n",
    "plotHistogram(dataset,\"Outcome\",'Insulin','Insulin vs Diagnosis (Blue = Healthy; Orange = Diabetes)')\n",
    "plotHistogram(dataset,\"Outcome\",'SkinThickness','SkinThickness vs Diagnosis (Blue = Healthy; Orange = Diabetes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb8374",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We will optimize the hyperparameters of the best-performing model using GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "6c9eff67-66d0-46c4-80ea-3e2451471683",
    "_uuid": "0031b33c44f66dfc7437586c0b7a3e466979c42f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset2 = dataset.iloc[:, :-1]\n",
    "print(\"# of Rows, # of Columns: \",dataset2.shape)\n",
    "print(\"\\nColumn Name           # of Null Values\\n\")\n",
    "print((dataset2[:] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "4751b521-e977-46d6-948c-de0bf90e6082",
    "_uuid": "6c8a90ee5f18fd5e214eec426ae38802c66503e1"
   },
   "outputs": [],
   "source": [
    "print(\"# of Rows, # of Columns: \",dataset2.shape)\n",
    "print(\"\\nColumn Name              % Null Values\\n\")\n",
    "print(((dataset2[:] == 0).sum())/768*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "694d1b57-4b0c-45b3-a266-2c672895445c",
    "_uuid": "ae01a03b7471670b369afb918679cf932ea9e6d9"
   },
   "source": [
    "Approximately 50% of the patients did not have their insulin levels measured.  This causes me to be concerned that maybe the doctors only measured insulin levels in unhealthy looking patients -- or maybe they only measured insulin levels after having first made a preliminary diagnosis.  If that were true then this would be a form of [data leakage](https://www.kaggle.com/dansbecker/data-leakage), and it would mean that our model would not generalize well to data collected from doctors who measure insulin levels for every patient.  In order to test this hypothesis I will check whether or not the Insulin and SkinThickness features are correlated with a diagnostic outcome (healthy/diabetic).  What we find is that these features are not highly correlated with any given outcome -- and as such we can rule out our concern of data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "d9459be8-3ac9-477c-846b-471ce09cda23",
    "_uuid": "c2039ea71625f74a46452421ecd2ada3618465fa"
   },
   "outputs": [],
   "source": [
    "g = sns.heatmap(dataset.corr(),cmap=\"BrBG\",annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c8d41550-c10e-4b7d-b5fd-7baa1c4a8560",
    "_uuid": "39901d449b5617ce676357f85c85a051b2b0fe6b"
   },
   "outputs": [],
   "source": [
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "63397347-a145-4fc0-b8a9-2cb06837f448",
    "_uuid": "bde3a1afd9a3a74efe90bb3b121b7cd5ac8c70e8"
   },
   "source": [
    "The Insulin and SkinThickness measurements are not highly correlated with any given outcome -- and as such we can rule out our concern of data leakage.  The zero values in these categories are still erroneous, however, and therefore should not be included in our model.  It is best to replace these values with some distribution of values near to the median measurement.  Also note that it is best to impute these values *after* the [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function in order to prevent yet another form of data leakage (i.e. the testing data should not be used when calculating the median value to use during imputation).  The following histogram illustrates that the null values have indeed been replaced with median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8aac39aa-0047-4963-ad7a-f1b98e92f4f5",
    "_uuid": "305e3128b63c6ec838c9c804ffc7e5bf7ef54b92"
   },
   "outputs": [],
   "source": [
    "data = read_csv('../input/diabetes.csv')\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "imputer = Imputer(missing_values=0,strategy='median')\n",
    "X_train2 = imputer.fit_transform(X_train)\n",
    "X_test2 = imputer.transform(X_test)\n",
    "X_train3 = pd.DataFrame(X_train2)\n",
    "plotHistogram(X_train3,None,4,'Insulin vs Diagnosis (Blue = Healthy; Orange = Diabetes)')\n",
    "plotHistogram(X_train3,None,3,'SkinThickness vs Diagnosis (Blue = Healthy; Orange = Diabetes)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "0a5497a2-c66d-4e8a-936b-317e5bb3ee69",
    "_uuid": "2f9b3686bc6a2cfc70379cb76872454f7d871f0b"
   },
   "outputs": [],
   "source": [
    "labels = {0:'Pregnancies',1:'Glucose',2:'BloodPressure',3:'SkinThickness',4:'Insulin',5:'BMI',6:'DiabetesPedigreeFunction',7:'Age'}\n",
    "print(labels)\n",
    "print(\"\\nColumn #, # of Zero Values\\n\")\n",
    "print((X_train3[:] == 0).sum())\n",
    "# data[:] = data[:].replace(0, np.NaN)\n",
    "# print(\"\\nColumn #, # of Null Values\\n\")\n",
    "# print(np.isnan(X_train3).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ffa86ef6-7a50-4110-aea0-198d208134c5",
    "_uuid": "1047f9eab8fd3ba251509aeb7147d678fb8a58fd"
   },
   "source": [
    "*Step 4: Evaluate Classification Models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1743b613-a0b7-4371-af2d-ba22ee28f839",
    "_uuid": "289dab0f5b01ad7713d1571b9998d6a216caaed2"
   },
   "source": [
    "Because we have replaced all of the erroneous, missing, and null values with median values we are now ready to train and evaluate our models for predicting diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "f42c3b2f-7926-4d4d-bb14-cfd2088cc29c",
    "_uuid": "e4fc9b2ffc3999bf85badc844ec7d1bda9df07ba"
   },
   "outputs": [],
   "source": [
    "compareABunchOfDifferentModelsAccuracy(X_train2, y_train, X_test2, y_test)\n",
    "defineModels()\n",
    "# iterate over classifiers; adapted from https://www.kaggle.com/hugues/basic-ml-best-of-10-classifiers\n",
    "results = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    scores = cross_val_score(clf, X_train2, y_train, cv=5)\n",
    "    results[name] = scores\n",
    "for name, scores in results.items():\n",
    "    print(\"%20s | Accuracy: %0.2f%% (+/- %0.2f%%)\" % (name, 100*scores.mean(), 100*scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb320556-96f2-4ee3-b8c5-5d7c2e4dea2a",
    "_uuid": "da888d8ba44fd872cbd6365a34fc15b9f962a72a"
   },
   "source": [
    "*Step 5: Examine Decision Tree Model in More Detail*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "139c8318-7534-429a-95b8-93cfa50889fe",
    "_uuid": "33b00d6abeafbb0dae97a359ad690f4e42da9ede"
   },
   "source": [
    "Next let's explore the decision tree model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "8a7f778a-1191-48e9-818b-d6914ab1dfd9",
    "_kg_hide-input": true,
    "_uuid": "b61c2106275d1d58033b65a1a0d8c6067dbf7b03"
   },
   "outputs": [],
   "source": [
    "def runDecisionTree(a, b, c, d):\n",
    "    model = DecisionTreeClassifier()\n",
    "    accuracy_scorer = make_scorer(accuracy_score)\n",
    "    model.fit(a, b)\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "    accuracy = model_selection.cross_val_score(model, a, b, cv=kfold, scoring='accuracy')\n",
    "    mean = accuracy.mean() \n",
    "    stdev = accuracy.std()\n",
    "    prediction = model.predict(c)\n",
    "    cnf_matrix = confusion_matrix(d, prediction)\n",
    "    #plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,title='Normalized confusion matrix')\n",
    "    plot_learning_curve(model, 'Learning Curve For DecisionTreeClassifier', a, b, (0.60,1.1), 10)\n",
    "    #learning_curve(model, 'Learning Curve For DecisionTreeClassifier', a, b, (0.60,1.1), 10)\n",
    "    plt.show()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=dict_characters,title='Confusion matrix')\n",
    "    plt.show()\n",
    "    print('DecisionTreeClassifier - Training set accuracy: %s (%s)' % (mean, stdev))\n",
    "    return\n",
    "runDecisionTree(X_train2, y_train, X_test2, y_test)\n",
    "feature_names1 = X.columns.values\n",
    "def plot_decision_tree1(a,b):\n",
    "    dot_data = tree.export_graphviz(a, out_file=None, \n",
    "                             feature_names=b,  \n",
    "                             class_names=['Healthy','Diabetes'],  \n",
    "                             filled=False, rounded=True,  \n",
    "                             special_characters=False)  \n",
    "    graph = graphviz.Source(dot_data)  \n",
    "    return graph \n",
    "clf1 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\n",
    "clf1.fit(X_train2, y_train)\n",
    "plot_decision_tree1(clf1,feature_names1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "66b90423-8b54-4c88-93ea-2f56ab4ccb68",
    "_uuid": "bb7735c74d944489e2ad2bff3c2b24e7e33ee555"
   },
   "source": [
    "*Step 6: Evaluate Feature Importances*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04491720-b03a-45c4-b5f7-075079a76564",
    "_uuid": "be75ff5780480f1615c8db3fc0a4bfb9b7a28fa0"
   },
   "source": [
    "Many [kernel authors](https://www.kaggle.com/uciml/pima-indians-diabetes-database/kernels) have neglected to deal with the null values and missing data discussed in this notebook.  This mistake did not actually have much of an impact on the performance of most of their models, however, because, as it happens, the Insulin and SkinThickness measurements are actually very poor predictors and are assigned low feature importances as compared to features such as blood glucose levels and body mass index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "73925719-607e-4041-9012-4890c5e28867",
    "_kg_hide-input": true,
    "_uuid": "a4fafaf035a47504df06e17a6b0ba539957d8cc2"
   },
   "outputs": [],
   "source": [
    "feature_names = X.columns.values\n",
    "clf1 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\n",
    "clf1.fit(X_train2, y_train)\n",
    "print('Accuracy of DecisionTreeClassifier: {:.2f}'.format(clf1.score(X_test2, y_test)))\n",
    "columns = X.columns\n",
    "coefficients = clf1.feature_importances_.reshape(X.columns.shape[0], 1)\n",
    "absCoefficients = abs(coefficients)\n",
    "fullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\n",
    "print('DecisionTreeClassifier - Feature Importance:')\n",
    "print('\\n',fullList,'\\n')\n",
    "\n",
    "feature_names = X.columns.values\n",
    "clf2 = RandomForestClassifier(max_depth=3,min_samples_leaf=12)\n",
    "clf2.fit(X_train2, y_train)\n",
    "print('Accuracy of RandomForestClassifier: {:.2f}'.format(clf2.score(X_test2, y_test)))\n",
    "columns = X.columns\n",
    "coefficients = clf2.feature_importances_.reshape(X.columns.shape[0], 1)\n",
    "absCoefficients = abs(coefficients)\n",
    "fullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\n",
    "print('RandomForestClassifier - Feature Importance:')\n",
    "print('\\n',fullList,'\\n')\n",
    "\n",
    "clf3 = XGBClassifier()\n",
    "clf3.fit(X_train2, y_train)\n",
    "print('Accuracy of XGBClassifier: {:.2f}'.format(clf3.score(X_test2, y_test)))\n",
    "columns = X.columns\n",
    "coefficients = clf3.feature_importances_.reshape(X.columns.shape[0], 1)\n",
    "absCoefficients = abs(coefficients)\n",
    "fullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\n",
    "print('XGBClassifier - Feature Importance:')\n",
    "print('\\n',fullList,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "deb59ccf-c7cd-4a0d-a37e-33ef28987f88",
    "_uuid": "fecb03d811c12a928a4ed9199a4b1532834f9a28",
    "collapsed": true
   },
   "source": [
    "In the end we were able to predict diabetes from medical records with an accuracy of approximately 82%.  This was done by using tree-based classifiers that focus on important features such as blood glucose levels and body mass index.  In fact, we only lose 5% accuracy by dropping all of the data except for blood glucose levels and body mass index (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "257d3eca-95c0-4fdf-90d2-589c1f34985c",
    "_kg_hide-input": true,
    "_uuid": "420c5fd425ff2de2bde384df9f5874b1f1ed025b"
   },
   "outputs": [],
   "source": [
    "data = read_csv('../input/diabetes.csv')\n",
    "data2 = data.drop(['Pregnancies','BloodPressure','DiabetesPedigreeFunction', 'Age','SkinThickness','Insulin'], axis=1)\n",
    "X2 = data2.iloc[:, :-1]\n",
    "y2 = data2.iloc[:, -1]\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X2, y2, test_size=0.2, random_state=1)\n",
    "imputer = Imputer(missing_values=0,strategy='median')\n",
    "X_train3 = imputer.fit_transform(X_train3)\n",
    "X_test3 = imputer.transform(X_test3)\n",
    "clf4 = XGBClassifier()\n",
    "clf4.fit(X_train3, y_train3)\n",
    "print('Accuracy of XGBClassifier in Reduced Feature Space: {:.2f}'.format(clf4.score(X_test3, y_test3)))\n",
    "columns = X2.columns\n",
    "coefficients = clf4.feature_importances_.reshape(X2.columns.shape[0], 1)\n",
    "absCoefficients = abs(coefficients)\n",
    "fullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\n",
    "print('\\nXGBClassifier - Feature Importance:')\n",
    "print('\\n',fullList,'\\n')\n",
    "\n",
    "clf3 = XGBClassifier()\n",
    "clf3.fit(X_train2, y_train)\n",
    "print('\\n\\nAccuracy of XGBClassifier in Full Feature Space: {:.2f}'.format(clf3.score(X_test2, y_test)))\n",
    "columns = X.columns\n",
    "coefficients = clf3.feature_importances_.reshape(X.columns.shape[0], 1)\n",
    "absCoefficients = abs(coefficients)\n",
    "fullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\n",
    "print('XGBClassifier - Feature Importance:')\n",
    "print('\\n',fullList,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85815bfb-2bcc-4a54-9bf4-60498f8060d1",
    "_uuid": "2b2051010b7b7fa0607a954a5c036ffd0202536c"
   },
   "source": [
    "*Step 7: Summarize Results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64798bc2-59be-4ee2-995b-4a70c3c8f1e2",
    "_uuid": "a39de037bd996485391a5a6faee0cf25619cf6cd"
   },
   "source": [
    "In this notebook we predicted diabetes from medical records with an accuracy of approximately 82%  -- and we also discussed topics such as missing data, data imputation, and feature importances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
